{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19cd3df8",
   "metadata": {},
   "source": [
    "### Understanding gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32fbfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent function\n",
    "def gradient_descent(gradient, start, learning_rate, n_iter):\n",
    "    vector = start\n",
    "    for i in range(n_iter):\n",
    "        step_size = learning_rate * gradient(vector)\n",
    "        vector = vector - step_size\n",
    "    return vector\n",
    "\n",
    "# gradient ==> callable object\n",
    "# start ==> initial point of search ==> tuple/list/numpy array/scalar\n",
    "# learning_rate ==> controls magnitude of vector update\n",
    "# n_iter ==> number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de391e",
   "metadata": {},
   "source": [
    "- If learning rate is too small, then the algorithm will converge very slowly\n",
    "- If learning rate is too big, it can make the algorithm too divergent\n",
    "\n",
    "Another termination criteria is **Tolerance**. The step size should be greater than a minimum threshold (tolerance). If the step size is too small, i.e. tends to zero, then it means that the slope or the gradient is also tending to zero. This indicates that we are close to our local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23666fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent with tolerance\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gradient_descent2(gradient, start, learning_rate, n_iter, tolerance):\n",
    "    vector = start\n",
    "    for i in range(n_iter):\n",
    "        step_size = learning_rate * gradient(vector)\n",
    "        # since each vector has multiple parameters, we compare all the elements of the numpy array\n",
    "        # i.e. step_size for all parameters\n",
    "        if np.all(np.abs(step_size) <= tolerance):\n",
    "            break\n",
    "        vector = vector - step_size\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12dc75",
   "metadata": {},
   "source": [
    "**Example 1.** \n",
    "\n",
    "f(v) = v^2\n",
    "\n",
    "gradient of f(v) = 2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8770ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0370359763344878e-09"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only one parameter v\n",
    "# testing the gradient_descent function on v^2\n",
    "# gradient or derivative here is 2v\n",
    "\n",
    "gradient_descent( gradient = lambda v : 2*v, start = 10.0, learning_rate = 0.1, n_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc68668",
   "metadata": {},
   "source": [
    "**Example 2.** \n",
    "\n",
    "f(v1, v2) = v1^2 + v2^4\n",
    "\n",
    "gradient of f(v1, v2) = 2*v1 + 4*v2^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc8de9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.41661026e-222, 2.47806975e-002])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 parameters v1 and v2\n",
    "# function is v1^2 + v2^4\n",
    "# gradient = 2*v1 + 4*v2^3\n",
    "\n",
    "gradient_descent2(gradient=lambda v: np.array([2 * v[0], 4 * v[1]**3]),\n",
    "                  start=np.array([1.0, 1.0]),\n",
    "                  learning_rate=0.2,\n",
    "                  n_iter = 1000,\n",
    "                  tolerance=1e-08)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e557f5",
   "metadata": {},
   "source": [
    "The above example correctly converges at v1 = v2 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eefea4",
   "metadata": {},
   "source": [
    "#### Residual sum of squares (RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99acedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "# x(input) and y(output) are observations from the dataset\n",
    "# b is the vector with all parameters\n",
    "\n",
    "def rss_gradient(x,y,b):\n",
    "    res = b[0] + b[1]*x - y\n",
    "    return 2*res.mean(), 2*(res*x).mean() # returns a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1d64417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new gradient descent with x and y inputs\n",
    "# start gives the value for initally assumed values for the parameters\n",
    "\n",
    "def gradient_descent3( gradient, x, y, start, learning_rate =0.1, n_iter = 50, tolerance = 1e-06):\n",
    "    # starting point\n",
    "    vector = start\n",
    "    for i in range(n_iter):\n",
    "        # converting output tuple from \"gradient function\" to array\n",
    "        # enables elementwise multiplication\n",
    "        step_size = learning_rate * np.array(gradient(x, y, vector))\n",
    "        if(np.all(np.abs(step_size)) <= tolerance):\n",
    "            break\n",
    "        vector = vector - step_size\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2485e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([5, 15, 25, 35, 45, 55])\n",
    "y = np.array([5, 20, 14, 32, 22, 38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbcd75a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.63333333, 0.54      ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent3( rss_gradient, x, y, start = [0.5, 0.5], learning_rate = 0.0008, n_iter = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36fcd66",
   "metadata": {},
   "source": [
    "The parameters corresponding to the best fit line are b0 = 5.63 and b1 = 0.54\n",
    "\n",
    "The equation for the best fit line is f(x) = 5.63 + 0.54x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805574f",
   "metadata": {},
   "source": [
    "##### More general function for gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21fda239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent( gradient, x, y, start, learning_rate = 0.1, n_iter = 100, tolerance = 1e-06, \n",
    "                     dtype = \"float64\"):\n",
    "    # checking edge cases for all arguments\n",
    "    \n",
    "    # setting up consistent data type for all arguments\n",
    "    dtype_ = np.dtype(dtype)\n",
    "    \n",
    "    # check if gradient is callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable!\")\n",
    "    \n",
    "    # converting x(inputs) and y(outputs) to Numpy arrays of float datatype\n",
    "    x, y = np.array(x, dtype_) , np.array(y, dtype_)\n",
    "    # dimensions of x and y should be equal\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"Dimensions of x and y should match!\")\n",
    "    \n",
    "    # settin up learning_rate as numpy array of float datatype\n",
    "    learning_rate = np.array(learning_rate, dtype = dtype_)\n",
    "    if np.any(learning_rate <= 0):\n",
    "        raise ValueError(\"'learning_rate' must be greater than zero!\")\n",
    "    \n",
    "    # checking number of iterations\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter < 0:\n",
    "        raise ValueError(\"No. of iterations should be greater than 0!\")\n",
    "        \n",
    "    # setting up tolerance as numpy array of float datatype\n",
    "    tolerance = np.array(tolerance, dtype = dtype_)\n",
    "    if np.any(tolerance <= 0):\n",
    "        raise ValueError(\"'tolerance' must be greater than zero!\")\n",
    "        \n",
    "    vector = np.array(start, dtype = dtype_)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        step_size = learning_rate * np.array(gradient(x, y, vector), dtype_)\n",
    "        if np.all(np.abs(step_size) <= tolerance):\n",
    "            break\n",
    "        vector = vector - step_size\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7653e",
   "metadata": {},
   "source": [
    "#### Types of Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d0dff",
   "metadata": {},
   "source": [
    "- **Online stochastic gradient descent**: is a variant in which we estimate the gradient of cost function for all observations (i.e. each **epoch**/iteration goes through the entire dataset) and updates the paramters of the cost function accordingly. Helps find the global minimum (among multiple local minimums). \n",
    "\n",
    "- **Batch stochastic gradient descent**: we take subsets of all observations into **mini-batches**. Gradients are calculated and the parameters of the loss functions are updated iteratively. Online stochastic gradient could be thought of as a special case of batch stochastic where there's only one batch taking all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b2d65",
   "metadata": {},
   "source": [
    "##### What happens inside the iterations?\n",
    "\n",
    "1. Randomly divides set of observations into **mini-batches.** \n",
    "2. For each minibatch, gradient is computed and the parameter vector is updated.\n",
    "3. Once all minibatches are used, it means one iteration or **epoch** is finished, move on to the next epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b38800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sgd( gradient, x, y, start, learning_rate = 0.1, n_iter = 100, tolerance = 1e-06, \n",
    "              dtype = \"float64\", batch_size = 1, random_state = None ):\n",
    "    \n",
    "    # gradient must be callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable!\")\n",
    "    \n",
    "    # consistent datatypes\n",
    "    dtype_ = np.dtype(dtype)\n",
    "    \n",
    "    # coverting x(inputs) and y(outputs) to array with datatype as above\n",
    "    x, y = np.array(x, dtype = dtype_) , np.array(y, dtype = dtype_)\n",
    "    # number of observations, number of inputs and number of outputs should be same\n",
    "    n_obs = x.shape[0]\n",
    "    if n_obs != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match!\")\n",
    "    \n",
    "    # concatenate the x and y into a single array\n",
    "    \n",
    "    # transform x into a line vector with n observations\n",
    "    # transform y into a column with n observation \n",
    "    # https://stackoverflow.com/questions/57962718/reshaping-data-in-numpy-with-1-1-what-does-it-mean\n",
    "    \n",
    "    xy = np.c_[ x.reshape(n_obs, -1), y.reshape(n_obs, 1) ]\n",
    "    \n",
    "    # learning_rate\n",
    "    learning_rate = np.array(learning_rate, dtype = dtype_)\n",
    "    if np.any(learning_rate) <= 0:\n",
    "        raise ValueError(\"'learning_rate' must be greater than 0!\")\n",
    "    \n",
    "    # n_iter\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than 0!\")\n",
    "    \n",
    "    # tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <=0):\n",
    "        raise ValueError(\"'tolerance' must be greater than 0!\")\n",
    "        \n",
    "    # initializing parameter vector\n",
    "    vector = np.array(start, dtype = dtype_)\n",
    "    \n",
    "    # intializing random number generator to set mini-batches\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    randnumgenerator = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    # setting up batch size\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\"'batch_size' needs to be greater than zero and less than the number of oberservations\")\n",
    "    \n",
    "# Performing batch gradient descent\n",
    "    for i in range(n_iter):\n",
    "        # shuffle xy to randomly select obeservations in batches for every iteration\n",
    "        randnumgenerator.shuffle(xy)\n",
    "        \n",
    "        # cover n_obs observations, repeated for each mini batch\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            \n",
    "            # select batches from start to stop index in xy, then break into x and y batch indiviually\n",
    "            x_batch = xy[start:stop, :-1] # first element of all arrays of xy\n",
    "            y_batch = xy[start:stop, -1:] # last element of all arrays of xy\n",
    "            \n",
    "            # Calculating step_size\n",
    "            \n",
    "            # gradient\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype = dtype_)\n",
    "            \n",
    "            # step_size \n",
    "            step_size = learning_rate*grad\n",
    "            \n",
    "            # check with tolerance\n",
    "            if np.all(np.abs(step_size) <= tolerance):\n",
    "                break\n",
    "            \n",
    "            vector = vector - step_size\n",
    "        \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc6eb7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.64931198, 0.81599011])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "batch_sgd( rss_gradient, x, y, start=[0.5, 0.5], learning_rate=0.0008, \n",
    "    batch_size=3, n_iter=100_000, random_state=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac60184",
   "metadata": {},
   "source": [
    "##### Momentum in stochastic gradient descent\n",
    "\n",
    "Corrects the effect of learning rate for calculating the step size. The idea is to remember the previous step_size update of the parameter vector and apply it when calculating the next one. You don't exactly move the vector exactly in the direction of the negative gradient, but you also use the direction and magnitude of the previous move.\n",
    "\n",
    "It helps to find the global minimum, from a bunch of local minimums since you do not get trapped in any particular local minimum. You are always pushed or pull out of the local minimums, until the gradient actually approaches zero.\n",
    "\n",
    "The additional parameter used for Momentum is called the decay rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e90deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_sgd( gradient, x, y, start, learning_rate = 0.1, n_iter = 100, tolerance = 1e-06, \n",
    "              dtype = \"float64\", batch_size = 1, random_state = None, decay_rate = 0.0 ):\n",
    "    \n",
    "    # gradient must be callable\n",
    "    if not callable(gradient):\n",
    "        raise TypeError(\"'gradient' must be callable!\")\n",
    "    \n",
    "    # consistent datatypes\n",
    "    dtype_ = np.dtype(dtype)\n",
    "    \n",
    "    # coverting x(inputs) and y(outputs) to array with datatype as above\n",
    "    x, y = np.array(x, dtype = dtype_) , np.array(y, dtype = dtype_)\n",
    "    # number of observations, number of inputs and number of outputs should be same\n",
    "    n_obs = x.shape[0]\n",
    "    if n_obs != y.shape[0]:\n",
    "        raise ValueError(\"'x' and 'y' lengths do not match!\")\n",
    "    \n",
    "    # concatenate the x and y into a single array\n",
    "    \n",
    "    # transform x into a line vector with n observations\n",
    "    # transform y into a column with n observation \n",
    "    # https://stackoverflow.com/questions/57962718/reshaping-data-in-numpy-with-1-1-what-does-it-mean\n",
    "    \n",
    "    xy = np.c_[ x.reshape(n_obs, -1), y.reshape(n_obs, 1) ]\n",
    "    \n",
    "    # learning_rate\n",
    "    learning_rate = np.array(learning_rate, dtype = dtype_)\n",
    "    if np.any(learning_rate) <= 0:\n",
    "        raise ValueError(\"'learning_rate' must be greater than 0!\")\n",
    "    \n",
    "    # n_iter\n",
    "    n_iter = int(n_iter)\n",
    "    if n_iter <= 0:\n",
    "        raise ValueError(\"'n_iter' must be greater than 0!\")\n",
    "    \n",
    "    # tolerance\n",
    "    tolerance = np.array(tolerance, dtype=dtype_)\n",
    "    if np.any(tolerance <=0):\n",
    "        raise ValueError(\"'tolerance' must be greater than 0!\")\n",
    "        \n",
    "    # initializing parameter vector\n",
    "    vector = np.array(start, dtype = dtype_)\n",
    "    \n",
    "    # intializing random number generator to set mini-batches\n",
    "    seed = None if random_state is None else int(random_state)\n",
    "    randnumgenerator = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    # setting up batch size\n",
    "    batch_size = int(batch_size)\n",
    "    if not 0 < batch_size <= n_obs:\n",
    "        raise ValueError(\"'batch_size' needs to be greater than zero and less than the number of oberservations\")\n",
    "    \n",
    "    # decay_rate\n",
    "    decay_rate = np.array(decay_rate, dtype = dtype_)\n",
    "    if np.any(decay_rate) < 0 or np.any(decay_rate) > 1:\n",
    "        raise ValueError(\"'decay_rate' must be between 0 and 1!\")\n",
    "        \n",
    "    # initialize the step_size \n",
    "    step_size = 0\n",
    "    \n",
    "# Performing batch gradient descent\n",
    "    for i in range(n_iter):\n",
    "        # shuffle xy to randomly select obeservations in batches for every iteration\n",
    "        randnumgenerator.shuffle(xy)\n",
    "        \n",
    "        # cover n_obs observations, repeated for each mini batch\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            \n",
    "            # select batches from start to stop index in xy, then break into x and y batch indiviually\n",
    "            x_batch = xy[start:stop, :-1] # first element of all arrays of xy\n",
    "            y_batch = xy[start:stop, -1:] # last element of all arrays of xy\n",
    "            \n",
    "            # Calculating step_size\n",
    "            \n",
    "            # gradient\n",
    "            grad = np.array(gradient(x_batch, y_batch, vector), dtype = dtype_)\n",
    "            \n",
    "            # step_size \n",
    "            step_size = learning_rate*grad - step_size*decay_rate\n",
    "            \n",
    "            # check with tolerance\n",
    "            if np.all(np.abs(step_size) <= tolerance):\n",
    "                break\n",
    "            \n",
    "            vector = vector - step_size\n",
    "        \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45685a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
